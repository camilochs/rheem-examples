{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 13 artifact(s)\n",
      "Adding 4 artifact(s)\n",
      "Adding 0 artifact(s)\n",
      "Adding 1 artifact(s)\n",
      "Adding 0 artifact(s)\n",
      "Adding 37 artifact(s)\n",
      "Adding 2 artifact(s)\n",
      "Adding 30 artifact(s)\n",
      "Adding 3 artifact(s)\n",
      "Adding 84 artifact(s)\n",
      "Adding 0 artifact(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mrheemVersion\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"0.2.0-SNAPSHOT\"\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Get the right repositories for SNAPSHOT versions.\n",
    "classpath.addRepository(\"file:///Users/basti/.m2/repository\")\n",
    "classpath.addRepository(\"https://oss.sonatype.org/content/repositories/snapshots\")\n",
    "\n",
    "val rheemVersion = \"0.2.0-SNAPSHOT\"\n",
    "\n",
    "// Load Rheem's core functionality.\n",
    "classpath.add(\"org.qcri.rheem\" % \"rheem-core\" % rheemVersion)\n",
    "classpath.add(\"org.qcri.rheem\" % \"rheem-api\" % rheemVersion)\n",
    "\n",
    "// Load Rheem's platform plugins.\n",
    "classpath.add(\"org.qcri.rheem\" % \"rheem-java\" % rheemVersion)\n",
    "classpath.add(\"org.qcri.rheem\" % \"rheem-spark\" % rheemVersion)\n",
    "classpath.add(\"org.qcri.rheem\" % \"rheem-basic\" % rheemVersion)\n",
    "classpath.add(\"org.qcri.rheem\" % \"rheem-graphchi\" % rheemVersion)\n",
    "classpath.add(\"org.qcri.rheem\" % \"rheem-sqlite3\" % rheemVersion)\n",
    "\n",
    "// Load the platforms themselves.\n",
    "classpath.add(\"org.apache.hadoop\" % \"hadoop-common\" % \"2.2.0\")\n",
    "classpath.add(\"org.apache.hadoop\" % \"hadoop-hdfs\" % \"2.2.0\")\n",
    "classpath.add(\"org.apache.spark\" % \"spark-core_2.11\" % \"1.6.1\")\n",
    "\n",
    "// Load the profiling utility used by Rheem.\n",
    "classpath.add(\"de.hpi.isg\" % \"profiledb-store\" % \"0.1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/Users/basti/.coursier/cache/v1/https/repo1.maven.org/maven2/org/slf4j/slf4j-simple/1.7.13/slf4j-simple-1.7.13.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/Users/basti/.coursier/cache/v1/https/repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.6.1/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/Users/basti/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.SimpleLoggerFactory]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.qcri.rheem.api._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.qcri.rheem.basic.data._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.qcri.rheem.core.api._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.qcri.rheem.core.util._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.qcri.rheem.core.function.FunctionDescriptor._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.qcri.rheem.java.Java\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.qcri.rheem.spark.Spark\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.qcri.rheem.sqlite3.Sqlite3\u001b[0m\n",
       "\u001b[32mimport \u001b[36mde.hpi.isg.profiledb.store.model._\u001b[0m\n",
       "\u001b[32mimport \u001b[36mscala.collection.JavaConversions._\u001b[0m\n",
       "\u001b[32mimport \u001b[36mscala.collection.mutable\u001b[0m\n",
       "\u001b[36mconf\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mqcri\u001b[0m.\u001b[32mrheem\u001b[0m.\u001b[32mcore\u001b[0m.\u001b[32mapi\u001b[0m.\u001b[32mConfiguration\u001b[0m = Configuration[file:///Users/basti/Work/Temp/Rheem/rheem.properties]\n",
       "defined \u001b[32mfunction \u001b[36mprintStats\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Import relevant packages.\n",
    "import org.qcri.rheem.api._\n",
    "import org.qcri.rheem.basic.data._\n",
    "import org.qcri.rheem.core.api._\n",
    "import org.qcri.rheem.core.util._\n",
    "import org.qcri.rheem.core.function.FunctionDescriptor._\n",
    "import org.qcri.rheem.java.Java\n",
    "import org.qcri.rheem.spark.Spark\n",
    "import org.qcri.rheem.sqlite3.Sqlite3\n",
    "import de.hpi.isg.profiledb.store.model._\n",
    "\n",
    "import scala.collection.JavaConversions._\n",
    "import scala.collection.mutable\n",
    "\n",
    "// Create a configuration.\n",
    "val conf = new Configuration(\"file:///Users/basti/Work/Temp/Rheem/rheem.properties\")\n",
    "\n",
    "// Little utility to show some execution metadata.\n",
    "def printStats(experiment: Experiment) {\n",
    "    def getTime(id: String) = experiment.getMeasurements.find(_.getId == id) match {\n",
    "        case Some(measurement) => measurement.asInstanceOf[TimeMeasurement].getMillis\n",
    "        case _ => 0\n",
    "    }\n",
    "    val optTime = getTime(\"Optimization\")\n",
    "    val execTime = getTime(\"Execution\")\n",
    "    val loEstTime = getTime(\"Estimate 1 (lower)\")\n",
    "    val hiEstTime = getTime(\"Estimate 1 (upper)\")\n",
    "    \n",
    "    println(\"Statistics\")\n",
    "    println(\"==========\")\n",
    "    println(s\"The optimization took ${Formats.formatDuration(optTime)}\")\n",
    "    println(s\"The execution took ${Formats.formatDuration(execTime)}, while Rheem estimated ${Formats.formatDuration(loEstTime)} to ${Formats.formatDuration(hiEstTime)}.\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:///Users/basti/Work/Data/text/odyssey.txt has 7848 different words, for instance:\n",
      "'the' appears 7076 times\n",
      "'and' appears 5406 times\n",
      "'of' appears 3639 times\n",
      "'to' appears 3605 times\n",
      "'a' appears 2052 times\n",
      "'i' appears 2023 times\n",
      "'you' appears 1985 times\n",
      "'in' appears 1930 times\n",
      "'he' appears 1893 times\n",
      "'for' appears 1433 times\n",
      "'his' appears 1343 times\n",
      "'as' appears 1316 times\n",
      "'that' appears 1316 times\n",
      "'it' appears 1284 times\n",
      "'with' appears 1249 times\n",
      "'him' appears 1079 times\n",
      "'was' appears 1065 times\n",
      "'is' appears 971 times\n",
      "'they' appears 971 times\n",
      "'on' appears 934 times\n",
      "'me' appears 921 times\n",
      "'had' appears 891 times\n",
      "'all' appears 887 times\n",
      "'my' appears 884 times\n",
      "'have' appears 880 times\n",
      "'but' appears 876 times\n",
      "'them' appears 795 times\n",
      "'not' appears 795 times\n",
      "'will' appears 737 times\n",
      "'so' appears 714 times\n",
      "'this' appears 693 times\n",
      "'her' appears 673 times\n",
      "'when' appears 667 times\n",
      "'ulysses' appears 655 times\n",
      "'your' appears 651 times\n",
      "'from' appears 647 times\n",
      "'she' appears 633 times\n",
      "'who' appears 632 times\n",
      "'are' appears 618 times\n",
      "'at' appears 602 times\n",
      "'be' appears 581 times\n",
      "'one' appears 573 times\n",
      "'then' appears 570 times\n",
      "'by' appears 566 times\n",
      "'we' appears 558 times\n",
      "'or' appears 525 times\n",
      "'were' appears 519 times\n",
      "'their' appears 494 times\n",
      "'said' appears 483 times\n",
      "'which' appears 431 times\n",
      "\n",
      "Statistics\n",
      "==========\n",
      "The optimization took 0:00:00.579\n",
      "The execution took 0:00:00.221, while Rheem estimated 0:00:00.212 to 0:00:00.731.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Wordcount\n",
    "locally {\n",
    "    val rheemContext = new RheemContext(conf)\n",
    "        .withPlugin(Java.basicPlugin)\n",
    "        .withPlugin(Spark.basicPlugin)\n",
    "    val planBuilder = new PlanBuilder(rheemContext)\n",
    "    val experiment = new Experiment(\"exp01\", new Subject(\"wordcount\", \"v1.0\"))\n",
    "    \n",
    "    val inputUrl = \"file:///Users/basti/Work/Data/text/odyssey.txt\"\n",
    "    import org.qcri.rheem.core.optimizer.ProbabilisticDoubleInterval\n",
    "    val wordsPerLine = new ProbabilisticDoubleInterval(5, 15, 0.9)\n",
    "    \n",
    "    val result = planBuilder\n",
    "      // Do some set up.\n",
    "      .withJobName(s\"WordCount ($inputUrl)\")\n",
    "      .withUdfJarsOf(this.getClass)\n",
    "      .withExperiment(experiment)\n",
    "\n",
    "      // Read the text file.\n",
    "      .readTextFile(inputUrl).withName(\"Load file\")\n",
    "\n",
    "      // Split each line by non-word characters.\n",
    "      .flatMap(_.split(\"\\\\W+\"), selectivity = wordsPerLine).withName(\"Split words\")\n",
    "\n",
    "      // Filter empty tokens.\n",
    "      .filter(_.nonEmpty, selectivity = 0.99).withName(\"Filter empty words\")\n",
    "\n",
    "      // Attach counter to each word.\n",
    "      .map(word => (word.toLowerCase, 1)).withName(\"To lower case, add counter\")\n",
    "\n",
    "      // Sum up counters for every word.\n",
    "      .reduceByKey(_._1, (c1, c2) => (c1._1, c1._2 + c2._2)).withName(\"Add counters\")\n",
    "      .withCardinalityEstimator((in: Long) => math.round(in * 0.01))\n",
    "\n",
    "      // Execute the plan and collect the results.\n",
    "      .collect()\n",
    "\n",
    "    println(s\"$inputUrl has ${result.size} different words, for instance:\")\n",
    "    result.toSeq.sortBy(-_._2).take(50).map(wc => s\"'${wc._1}' appears ${wc._2} times\").foreach(println)\n",
    "\n",
    "    println()\n",
    "    printStats(experiment)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 INDs:\n",
      "REGION[R_REGIONKEY] is included in NATION[N_REGIONKEY] and NATION[N_NATIONKEY].\n",
      "NATION[N_REGIONKEY] is included in REGION[R_REGIONKEY] and NATION[N_NATIONKEY].\n",
      "\n",
      "Statistics\n",
      "==========\n",
      "The optimization took 0:00:00.000\n",
      "The execution took 0:00:00.000, while Rheem estimated 0:00:00.000 to 0:00:00.000.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// SINDY (IND discovery)\n",
    "\n",
    "locally {\n",
    "    \n",
    "    // Parameters\n",
    "    val jdbcUrl = \"jdbc:sqlite:/Users/basti/Work/Temp/Rheem/tpch.db\"\n",
    "    val schemaDefPath = \"/Users/basti/Work/Temp/Rheem/tpch-schema.txt\"\n",
    "    val tables = Seq((\"REGION\", Seq(\"*\")), (\"NATION\", Seq(\"*\")))\n",
    "    \n",
    "    \n",
    "    // Define some of the more complex UDFs.\n",
    "    /**\n",
    "    * UDF to create cells from a [[Record]].\n",
    "    *\n",
    "    * @param offset the column ID offset for the input [[Record]]s\n",
    "    */\n",
    "    class CellCreator(val offset: Int) extends SerializableFunction[Record, java.lang.Iterable[(String, Int)]] {\n",
    "\n",
    "    override def apply(record: Record): java.lang.Iterable[(String, Int)] = {\n",
    "      val cells = new java.util.ArrayList[(String, Int)](record.size)\n",
    "      var columnId = offset\n",
    "      for (index <- 0 until record.size) {\n",
    "        cells.add((record.getString(index), columnId))\n",
    "        columnId += 1\n",
    "      }\n",
    "      cells\n",
    "    }\n",
    "  }\n",
    "\n",
    "  /**\n",
    "    * UDF to merge the column IDs of two cells.\n",
    "    */\n",
    "  class CellMerger extends SerializableBinaryOperator[(String, Array[Int])] {\n",
    "\n",
    "    lazy val merger = mutable.Set[Int]()\n",
    "\n",
    "    override def apply(cell1: (String, Array[Int]), cell2: (String, Array[Int])): (String, Array[Int]) = {\n",
    "      merger.clear()\n",
    "      for (columnId <- cell1._2) merger += columnId\n",
    "      for (columnId <- cell2._2) merger += columnId\n",
    "      (cell1._1, merger.toArray)\n",
    "    }\n",
    "  }\n",
    "\n",
    "  /**\n",
    "    * UDF to create IND candidates from a cell group.\n",
    "    */\n",
    "  class IndCandidateGenerator extends SerializableFunction[(String, Array[Int]), java.lang.Iterable[(Int, Array[Int])]] {\n",
    "\n",
    "    override def apply(cellGroup: (String, Array[Int])): java.lang.Iterable[(Int, Array[Int])] = {\n",
    "      val columnIds = cellGroup._2\n",
    "      val result = new java.util.ArrayList[(Int, Array[Int])](columnIds.length)\n",
    "      for (i <- columnIds.indices) {\n",
    "        val refColumnIds = new Array[Int](columnIds.length - 1)\n",
    "        java.lang.System.arraycopy(columnIds, 0, refColumnIds, 0, i)\n",
    "        java.lang.System.arraycopy(columnIds, i + 1, refColumnIds, i, refColumnIds.length - i)\n",
    "        result.add((columnIds(i), refColumnIds))\n",
    "      }\n",
    "      result\n",
    "    }\n",
    "  }\n",
    "\n",
    "  /**\n",
    "    * UDF to merge two IND candidates.\n",
    "    */\n",
    "  class IndCandidateMerger extends SerializableBinaryOperator[(Int, Array[Int])] {\n",
    "\n",
    "    lazy val merger = mutable.Set[Int]()\n",
    "\n",
    "    override def apply(indc1: (Int, Array[Int]), indc2: (Int, Array[Int])): (Int, Array[Int]) = {\n",
    "      merger.clear()\n",
    "      for (columnId <- indc1._2) merger += columnId\n",
    "      (indc1._1, indc2._2.filter(merger.contains))\n",
    "    }\n",
    "\n",
    "  }\n",
    "    \n",
    "    // Load the schema definition.\n",
    "    val tableRegex = \"\"\"(\\w+)\\[((?:\\w+(?:,\\w+)*)|(?:\\*))\\]\"\"\".r\n",
    "    val schema = scala.io.Source.fromFile(schemaDefPath).getLines().map(_ match {\n",
    "        case tableRegex(table, columns) => (table, columns.split(',').toSeq)\n",
    "        case other: String => sys.error(s\"Illegal table specifier: $other\")\n",
    "    }).toMap\n",
    "    \n",
    "    // Prepare the RheemContext.\n",
    "    val rheemContext = new RheemContext(conf)\n",
    "        .withPlugin(Java.basicPlugin)\n",
    "        .withPlugin(Spark.basicPlugin)\n",
    "        .withPlugin(Sqlite3.plugin)\n",
    "    rheemContext.getConfiguration.setProperty(\"rheem.sqlite3.jdbc.url\", jdbcUrl)\n",
    "    val experiment = new Experiment(\"exp02\", new Subject(\"sindy\", \"v1.0\"))\n",
    "    val planBuilder = new PlanBuilder(rheemContext)\n",
    "\n",
    "    // Create cells from the various tables.\n",
    "    var offset = 0\n",
    "    val columnsById = mutable.Map[Int, String]()\n",
    "    val allCells = tables\n",
    "      .map { case (table, columns) =>\n",
    "        // Handle the special case where columns == \"*\".\n",
    "        var resolvedColumns = if (columns == Seq(\"*\")) schema(table) else columns\n",
    "\n",
    "        // Read the cells from the specified table/columns.\n",
    "        var records = planBuilder.readTable(new org.qcri.rheem.sqlite3.operators.Sqlite3TableSource(table, schema(table): _*)).withName(s\"Load $table\")\n",
    "\n",
    "        // If requested, project to the relevant fields.\n",
    "        if (resolvedColumns.toSet != schema(table).toSet)\n",
    "          records = records.projectRecords(resolvedColumns).withName(s\"Project $table\")\n",
    "\n",
    "        // Create the cells, finally.\n",
    "        val cells = records.flatMapJava(new CellCreator(offset), selectivity = resolvedColumns.size.toDouble).withName(s\"Create cells for $table\")\n",
    "\n",
    "        // Maintain some helper data structures.\n",
    "        resolvedColumns.zipWithIndex.foreach { case (column, index) => columnsById(offset + index) = s\"$table[$column]\" }\n",
    "        offset += resolvedColumns.size\n",
    "\n",
    "        cells\n",
    "      }\n",
    "      .reduce(_ union _)\n",
    "\n",
    "    // Do the rest of the SINDY logic on the cells.\n",
    "    val rawInds = allCells\n",
    "      .map(cell => (cell._1, Array(cell._2))).withName(\"Prepare cell merging\")\n",
    "      .reduceByKeyJava(toSerializableFunction(_._1), new CellMerger).withName(\"Merge cells\")\n",
    "      .flatMapJava(new IndCandidateGenerator).withName(\"Generate IND candidate sets\")\n",
    "      .reduceByKeyJava(toSerializableFunction(_._1), new IndCandidateMerger).withName(\"Merge IND candidate sets\")\n",
    "      .filter(_._2.length > 0).withName(\"Filter empty candidate sets\")\n",
    "      .collect()\n",
    "\n",
    "    // Make the results readable.\n",
    "    val inds = rawInds.map {\n",
    "      case (dep, refs) => (s\"${columnsById(dep)}\", refs.map(columnsById).toSeq)\n",
    "    }\n",
    "    \n",
    "    println(s\"Found ${inds.map(_._2.length).sum} INDs:\")\n",
    "    inds.foreach { case (dep, refs) => println(s\"$dep is included in ${refs.mkString(\" and \")}.\")}\n",
    "    \n",
    "    println\n",
    "    printStats(experiment)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate page ranks for 659133 vertices:\n",
      "United_States has a page rank of 0.001323096.\n",
      "France has a page rank of 7.284153E-4.\n",
      "Germany has a page rank of 7.254262E-4.\n",
      "World_War_II has a page rank of 6.274851E-4.\n",
      "Category:American_people has a page rank of 5.342795E-4.\n",
      "Italy has a page rank of 4.8019184E-4.\n",
      "English_language has a page rank of 4.735535E-4.\n",
      "United_Kingdom has a page rank of 4.205242E-4.\n",
      "Switzerland has a page rank of 4.098024E-4.\n",
      "World_War_I has a page rank of 3.8821474E-4.\n",
      "Paris has a page rank of 3.7255522E-4.\n",
      "New_York_City has a page rank of 3.665075E-4.\n",
      "Category:German_people has a page rank of 3.5535655E-4.\n",
      "Austria has a page rank of 3.3793878E-4.\n",
      "London has a page rank of 3.3367594E-4.\n",
      "Canada has a page rank of 3.311628E-4.\n",
      "Berlin has a page rank of 3.3020417E-4.\n",
      "Russia has a page rank of 3.258669E-4.\n",
      "Association_football has a page rank of 3.1233847E-4.\n",
      "England has a page rank of 2.9962158E-4.\n",
      "Spain has a page rank of 2.9902352E-4.\n",
      "Latin has a page rank of 2.9132134E-4.\n",
      "Category:Writers has a page rank of 2.8897644E-4.\n",
      "California has a page rank of 2.731293E-4.\n",
      "Japan has a page rank of 2.7125422E-4.\n",
      "German_language has a page rank of 2.553056E-4.\n",
      "Poland has a page rank of 2.5427595E-4.\n",
      "India has a page rank of 2.3686688E-4.\n",
      "French_language has a page rank of 2.3386511E-4.\n",
      "U.S._state has a page rank of 2.3268582E-4.\n",
      "Netherlands has a page rank of 2.3079186E-4.\n",
      "Soviet_Union has a page rank of 2.2724643E-4.\n",
      "Sweden has a page rank of 2.1854966E-4.\n",
      "Category:Actors has a page rank of 2.1739691E-4.\n",
      "Actor has a page rank of 2.1558623E-4.\n",
      "Politician has a page rank of 2.1318553E-4.\n",
      "Catholic_Church has a page rank of 2.0500206E-4.\n",
      "Australia has a page rank of 2.0363448E-4.\n",
      "Europe has a page rank of 2.018954E-4.\n",
      "Person has a page rank of 1.9437057E-4.\n",
      "China has a page rank of 1.9305451E-4.\n",
      "Category:British_people has a page rank of 1.9034393E-4.\n",
      "Vienna has a page rank of 1.8953046E-4.\n",
      "Rome has a page rank of 1.8001557E-4.\n",
      "Denmark has a page rank of 1.790698E-4.\n",
      "Brazil has a page rank of 1.7606062E-4.\n",
      "United_States_dollar has a page rank of 1.7575025E-4.\n",
      "Ancient_Rome has a page rank of 1.728541E-4.\n",
      "Japanese_writing_system has a page rank of 1.6633516E-4.\n",
      "Turkey has a page rank of 1.6420626E-4.\n",
      "\n",
      "Statistics\n",
      "==========\n",
      "The optimization took 0:00:01.214\n",
      "The execution took 0:00:11.610, while Rheem estimated 0:00:14.409 to 0:52:44.521.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// PageRank on RDF data\n",
    "\n",
    "locally {\n",
    "    import org.qcri.rheem.api.graph._\n",
    "    \n",
    "    val inputUrl = \"file:///Users/basti/Work/Data/rdf/dbpedia/page-links-en-uris_de.sample_5pc.nt\"\n",
    "    val numIterations = 10\n",
    "    \n",
    "    val dbpediaEntity = \"\"\"<http://dbpedia.org/resource/(.+)>\"\"\".r\n",
    "    def parseEntity(url: String) = url match {\n",
    "        case dbpediaEntity(name) => name\n",
    "        case other => other\n",
    "    }\n",
    "    \n",
    "    // Initialize.\n",
    "    val rheemCtx = new RheemContext(conf)\n",
    "        .withPlugin(Java.basicPlugin)\n",
    "        .withPlugin(Java.graphPlugin)\n",
    "    val experiment = new Experiment(\"exp03\", new Subject(\"pagerank\", \"v1.0\"))\n",
    "    val planBuilder = new PlanBuilder(rheemCtx)\n",
    "      .withJobName(s\"PageRank ($inputUrl, $numIterations iterations)\")\n",
    "      .withUdfJarsOf(this.getClass)\n",
    "      .withExperiment(experiment)\n",
    "\n",
    "    // Read and parse the input file.\n",
    "    val edges = planBuilder\n",
    "      .readTextFile(inputUrl).withName(\"Load file\")\n",
    "      .filter(!_.startsWith(\"#\"), selectivity = 1.0).withName(\"Filter comments\")\n",
    "      .map { raw =>\n",
    "            // Find the first two spaces: Odds are that these are separate subject, predicated and object.\n",
    "            val firstSpacePos = raw.indexOf(' ')\n",
    "            val secondSpacePos = raw.indexOf(' ', firstSpacePos + 1)\n",
    "\n",
    "            // Find the end position.\n",
    "            var stopPos = raw.lastIndexOf('.')\n",
    "            while (raw.charAt(stopPos - 1) == ' ') stopPos -= 1\n",
    "\n",
    "            (raw.substring(0, firstSpacePos),\n",
    "             raw.substring(firstSpacePos + 1, secondSpacePos),\n",
    "             raw.substring(secondSpacePos + 1, stopPos))\n",
    "      }.withName(\"Parse triples\")\n",
    "      .map { case (s, p, o) => (parseEntity(s), parseEntity(o)) }.withName(\"Discard predicate\")\n",
    "\n",
    "    // Create vertex IDs.\n",
    "    val vertexIds = edges\n",
    "      .flatMap(edge => Seq(edge._1, edge._2)).withName(\"Extract vertices\")\n",
    "      .distinct.withName(\"Distinct vertices\")\n",
    "      .zipWithId.withName(\"Add vertex IDs\")\n",
    "\n",
    "    // Encode the edges with the vertex IDs\n",
    "    type VertexId = org.qcri.rheem.basic.data.Tuple2[Vertex, String]\n",
    "    val idEdges = edges\n",
    "      .join[VertexId, String](_._1, vertexIds, _.field1).withName(\"Join source vertex IDs\")\n",
    "      .map { linkAndVertexId =>\n",
    "        (linkAndVertexId.field1.field0, linkAndVertexId.field0._2)\n",
    "      }.withName(\"Set source vertex ID\")\n",
    "      .join[VertexId, String](_._2, vertexIds, _.field1).withName(\"Join target vertex IDs\")\n",
    "      .map(linkAndVertexId => new Edge(linkAndVertexId.field0._1, linkAndVertexId.field1.field0)).withName(\"Set target vertex ID\")\n",
    "\n",
    "    // Run the PageRank.\n",
    "    // Note: org.qcri.rheem.api.graph._ must be imported for this to work.\n",
    "    val pageRanks = idEdges.pageRank(numIterations)\n",
    "\n",
    "    // Make the page ranks readable.\n",
    "    val result = pageRanks\n",
    "      .map(identity).withName(\"Hotfix\")\n",
    "      .join[VertexId, Long](_.field0, vertexIds, _.field0).withName(\"Join page ranks with vertex IDs\")\n",
    "      .map(joinTuple => (joinTuple.field1.field1, joinTuple.field0.field1)).withName(\"Make page ranks readable\")\n",
    "      .collect()\n",
    "    \n",
    "    println(s\"Calculate page ranks for ${result.size} vertices:\")\n",
    "    result.toSeq.sortBy(-_._2).take(50).foreach(pr => println(s\"${pr._1} has a page rank of ${pr._2}.\"))\n",
    "    println\n",
    "    printStats(experiment)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
